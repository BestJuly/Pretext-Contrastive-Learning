# Good Practices in Self-supervised Video Feature Learning: Lifting Benckmarks to A New Level
This code shows a demo of experiments in our paper: Good Practices in Self-supervised Video Feature Learning: Lifting Benckmarks to A New Level.

This demo demonstrates one training setting out of many possible combinations: **PCL(VCP), R3D**. And our training strategies (residual clips and strong data augmentations) have been set as the default setting.

The codes for this entire project are in refactoring and made public soon.

## Reproducibility declaration
We have fixed possible random seed by using 
```python
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
random.seed(seed)
np.random.seed(seed)
os.environ['PYTHONHASHSEED'] = str(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
torch.cuda.manual_seed_all(seed)
```

**However, we can still not guarantee that with our provided code, you could achieve exact results as ours because of other influences such as version mismatch of some environment settings. It should be obvious to see the improvements and similar performances.**


## Requirements
> This is my experimental environment when preparing this demo code. 
- Ubuntu 18.04.4 LTS
- conda 4.8.4
- PyTorch 1.4.0
- python  3.8.3
- cuda 10.1
- accimage 


# Usage
## Data preparation
I used resized RGB frames from this [repo](https://github.com/feichtenhofer/twostreamfusion). Frames of videos in UCF101 and HMDB51 datasets can be downloaded directly without decoding.

> Tips: There is a folder called `TSP_Flows` inside `v_LongJump_j18_c03 folder` in UCF101 dataset and you may meet a problem if you do not handle this. One solution is to delete this folder.

The folder architecture is like `path/to/dataset/jpegs_256/video_id/frames.jpg`.

Then, you need to edit `datasets/ucf101.py` and `datasets/hmdb51.py` to specify the path for dataset. Please change `*_dataset_path` in line #19. 

## Training self-supervised learning part with our PCL
```
python train_vcp_contrast.py
```
Default settings are
- Method: PCL (VCP)
- Backbone: R3D
- Modality: Res
- Augmentation: RandomCrop, ColorJitter, RandomGrayScale, GaussianBlur, RandomHorizontalFlip
- Dataset: UCF101
- Split: 1

These settings are also fixed for the following process, we do not need to specify `--model=r3d --modality=res`.

> The training will take around 42 hours on one V100 based on our experimental environment.

Models will be saved to `./logs/exp_name`. Here, `exp_name` is directly generated by its corresponding settings.

## Evaluation using video retrieval
```
python retrieve_clips.py --ckpt=/path/to/ssl/best_model --dataset=ucf101
```

## Fine-tuning on video recognition
```
python ft_classify.py --ckpt=/path/to/ssl/best_model --dataset=ucf101
```
The testing process will automatically run after training is done.